<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AlphaWrite: Inference time compute Scaling for Writing - Tufa Labs Research Blog</title>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@400;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="/blog/assets/css/tufa-style.css">
    <link rel="stylesheet" href="/blog/assets/css/syntax.css">
    <link type="application/atom+xml" rel="alternate" href="https://tufalabs.ai/blog/feed.xml" title="Tufa Labs Research Blog" />
    <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>AlphaWrite: Inference time compute Scaling for Writing | Tufa Labs Research Blog</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="AlphaWrite: Inference time compute Scaling for Writing" />
<meta name="author" content="Toby Simonds" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="We introduce AlphaWrite, an inference-time scaling method for creative writing that uses evolutionary generation and ELO-based ranking to improve story quality." />
<meta property="og:description" content="We introduce AlphaWrite, an inference-time scaling method for creative writing that uses evolutionary generation and ELO-based ranking to improve story quality." />
<link rel="canonical" href="https://tufalabs.ai/blog/jekyll/update/2025/06/06/AlphaWrite.html" />
<meta property="og:url" content="https://tufalabs.ai/blog/jekyll/update/2025/06/06/AlphaWrite.html" />
<meta property="og:site_name" content="Tufa Labs Research Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2025-06-06T08:50:53+10:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="AlphaWrite: Inference time compute Scaling for Writing" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Toby Simonds"},"dateModified":"2025-06-06T08:50:53+10:00","datePublished":"2025-06-06T08:50:53+10:00","description":"We introduce AlphaWrite, an inference-time scaling method for creative writing that uses evolutionary generation and ELO-based ranking to improve story quality.","headline":"AlphaWrite: Inference time compute Scaling for Writing","mainEntityOfPage":{"@type":"WebPage","@id":"https://tufalabs.ai/blog/jekyll/update/2025/06/06/AlphaWrite.html"},"url":"https://tufalabs.ai/blog/jekyll/update/2025/06/06/AlphaWrite.html"}</script>
<!-- End Jekyll SEO tag -->

</head>
<body>
    <header>
        <div class="container">
            <div class="header-content">
                <a href="https://tufalabs.ai/" class="logo-container">
                    <img src="/blog/assets/images/tufa_logo.png" alt="Tufa Labs Logo" class="logo-image" id="logo-image">
                    <div class="logo-text">Tufa Labs</div>
                </a>
                <nav>
                    <ul>
                           <li><a href="https://tufalabs.ai/index.html">Home</a></li>
                        <li><a href="https://tufalabs.ai/blog/_site">Research</a></li>
                        <li><a href="https://tufalabs.ai/open_positions.html">Open Positions</a></li>
                        <li><a href="https://tufalabs.ai/team.html">Team</a></li>
                        <li><a href="https://tufalabs.ai/sponsor.html">MLST</a></li>
                    </ul>
                </nav>
            </div>
        </div>
    </header>
    <main class="container">
        
<article class="post">
    <header class="post-header">
        <h1 class="post-title">AlphaWrite: Inference time compute Scaling for Writing</h1>
        <div class="post-meta">
            <time datetime="2025-06-06T08:50:53+10:00">
                June 6, 2025
            </time>
            
            • <span class="post-author">Toby Simonds</span>
            
        </div>
    </header>

    <div class="post-content">
        <p>You can try AlphaWrite out here
<strong>Code Repository</strong>: <a href="https://github.com/tamassimonds/AlphaEvolveWritting">AlphaWrite on GitHub</a></p>

<p>Large language models have demonstrated remarkable improvements in performance through increased inference-time compute on quantitative reasoning tasks, particularly in mathematics and coding. However, the creative domain—where outputs are inherently highly subjective and difficult to evaluate—has seen limited exploration of systematic approaches to scale inference-time compute effectively.</p>

<p>In this work, we introduce Alpha Writing, a novel framework for scaling inference-time compute in creative text generation. Inspired by AlphaEvolve and other evolutionary algorithms, our approach combines iterative story generation with Elo-based evaluation to systematically improve narrative quality. Rather than relying on single-shot generation or simple resampling, Alpha Writing creates a dynamic ecosystem where stories compete, evolve, and improve through multiple generations.</p>

<p>Our method addresses a critical gap in the field: while we can easily scale compute for tasks with clear correctness criteria, creative domains have lacked principled approaches for leveraging additional inference resources. By treating story generation as an evolutionary process guided by pairwise preferences, we demonstrate that creative output quality can be systematically improved through increased compute allocation.</p>

<p>We further demonstrate the scalability of these methods by distilling the enhanced stories back into the base model, creating a stronger foundation for subsequent rounds of Alpha Writing. This recursive cycle—where improved outputs become training data for an enhanced model that can generate even better stories—offers promising potential for self improving writing models.</p>

<h1 id="methodology">Methodology</h1>

<p><img src="/blog/assets/images/AlphaWrite/AlphaWriteProcess.png" alt="image.png" /></p>

<h2 id="overview">Overview</h2>

<p>Alpha Writing employs an evolutionary approach to improve story quality through iterative generation and selection. The process consists of four main stages: (1) diverse initial story generation, (2) pairwise comparison using Elo rankings, and (3) evolutionary refinement of top-performing stories. (2) and (3) are repeated for multiple generations to progressively enhance narrative quality.</p>

<h2 id="initial-story-generation">Initial Story Generation</h2>

<p>To establish a diverse starting population, we generate a large corpus of initial stories with systematic variation. Each story is generated with two randomized parameters:</p>

<ul>
  <li><strong>Author style</strong>: The model is prompted to write in the style of different authors</li>
  <li><strong>Theme</strong>: Each generation focuses on a different narrative theme</li>
</ul>

<p>This approach ensures broad exploration of the creative space and prevents early convergence on a single narrative style or structure.</p>

<h2 id="judging-and-elo-ranking">Judging and Elo Ranking</h2>

<p>Stories are evaluated through pairwise comparisons using an LLM judge. The judge is provided with:</p>

<ul>
  <li>A detailed evaluation rubric focusing on narrative quality metrics</li>
  <li>Two stories to compare</li>
  <li>Instructions to select the superior story</li>
</ul>

<p>The rubric improves consistency in judgments by providing clear evaluation criteria. Based on these pairwise comparisons, we update Elo ratings for each story, creating a dynamic ranking system that captures relative quality differences. We use base Elo of 1200 and K-factor of 32. For our experiments we use the same model as the judge and generator</p>

<h2 id="story-evolution">Story Evolution</h2>

<p>After establishing rankings through pairwise comparisons, we implement an evolutionary process to iteratively improve story quality:</p>

<p><strong>1. Selection</strong>: Select top-performing stories as foundation for next generation</p>

<p><strong>2. Variation Generation</strong>: Generate variants using randomly sampled improvement objectives (narrative structure, character development, emotional resonance, dialogue, thematic depth, descriptive detail, plot tension, prose style). Random sampling maintains creative diversity.</p>

<p><strong>3. Population Update</strong>: Retain high-performers, replace lower-ranked stories with variants</p>

<p><strong>4. Re-ranking</strong>: Fresh pairwise comparisons on updated population</p>

<p><strong>5. Iteration</strong>: Repeat across generations, allowing successful elements to propagate</p>

<h2 id="evaluation-protocol">Evaluation Protocol</h2>

<p>Evaluating creative output presents significant challenges due to subjective preferences and high variance in story content. Our evaluation approach includes:</p>

<ul>
  <li><strong>Model selection</strong>: Focus on smaller models where improvements are more pronounced</li>
  <li><strong>Story length</strong>: Restrict to stories under 500 words to enable easier comparison</li>
  <li><strong>Prompt design</strong>: Use open-ended prompts to allow models to demonstrate narrative crafting abilities</li>
  <li><strong>Data collection</strong>: 120 preference comparisons per experiment to establish statistical significance</li>
  <li><strong>Evaluation Protocol:</strong> Evaluators same rubric we use for LLM judge to score which of the two responses they prefer</li>
</ul>

<p>Initial generations often exhibited fundamental narrative issues including poor story arcs and structural problems, making improvements through evolution particularly noticeable. We compare performance against initial model-generated stories and stories improved through repeated prompting.</p>

<p>We acknowledge that our evaluation methodology, while establishing statistically significant improvements, would benefit from more comprehensive data collection. We simply seek to demonstrate a statistically significant signal that this method works - quantifiying the actual improvement is difficult and would require significantly more diverse data colleciton</p>

<p>We found quality differences were subtle in opening lines but became pronounced in longer stories, where structural coherence and narrative flow showed clear improvement. However, evaluating these stories remains genuinely difficult—they diverge so dramatically in theme, style, and approach that determining which is “better” becomes largely subjective and dependent on reader preference.</p>

<h3 id="results">Results</h3>

<p>For evaluation we used Llama 3.1 8B and generated 60 initial stories, selected the top 5 performers, and created 5 variants of each. This evolution process was repeated for 5 generations</p>

<p><img src="/blog/assets/images/AlphaWrite/AlphaWriteBench.png" alt="image.png" /></p>

<p>Alpha Writing demonstrates substantial improvements in story quality when evaluated through pairwise human preferences. Testing with Llama 3.1 8B  revealed:</p>

<ul>
  <li><strong>72% preference rate</strong> over initial story generations (95 % CI 63 % – 79 %)</li>
  <li><strong>62% preference rate</strong> over sequential-prompting baseline (95 % CI 53 % – 70 %)</li>
</ul>

<p>These results indicate that the evolutionary approach significantly outperforms both single-shot generation and traditional inference-time scaling methods for creative writing tasks.</p>

<p><img src="/blog/assets/images/AlphaWrite/Demo.png" alt="image.png" /></p>

<h1 id="recursive-self-improvement-through-alphawrite-distillation">Recursive Self-Improvement Through AlphaWrite Distillation</h1>

<p>An intriguing possibility emerges when considering inference scaling techniques like AlphaEvolve or AlphaWrite: could we create a self improving loop through using inference scaling to improve results then distill back down and repeat?</p>

<h2 id="the-core-concept">The Core Concept</h2>

<p>The process would work as follows:</p>

<ol>
  <li>Apply AlphaWrite techniques to generate improved outputs from the current model</li>
  <li>Distill these enhanced outputs back into training data for the base model</li>
  <li>Reapply AlphaWrite techniques to this improved base, continuing the cycle</li>
</ol>

<h2 id="experiments">Experiments</h2>

<p>We explored this concept through preliminary testing:</p>

<ul>
  <li><strong>Generation Phase</strong>: Ran AlphaWrite with 60 initial questions, top 5 questions per batch, 5 variations of each for 5 generations. Ran process 10 times generating 50 stories in total</li>
  <li><strong>Selection</strong>: Identified the top 10 highest-quality stories of the final batch</li>
  <li><strong>Fine-tuning</strong>: Used these curated stories to fine-tune Llama 3.1 8B</li>
  <li><strong>Iteration</strong>: Repeated the process with the enhanced model</li>
</ul>

<p>This recursive approach theoretically enables continuous self-improvement, where each iteration builds upon the strengths of the previous generation, potentially leading to increasingly sophisticated capabilities without additional human-generated training data.</p>

<h2 id="results-1">Results</h2>

<p><img src="/blog/assets/images/AlphaWrite/Llama3.1Results.png" alt="image.png" /></p>

<p>We observed a 56%  (95 % CI 47 % – 65 %) preference rate over the base model. While this improvement falls within the statistical significance range for this experiment, collecting sufficient preference data to achieve statistical significance would be prohibitively expensive.</p>

<h2 id="limitations">Limitations</h2>

<p><strong>Prompt Sensitivity</strong>: The quality and diversity of generated stories are highly dependent on the specific prompts used. Our choice of author styles and themes introduces inherent bias that may favor certain narrative approaches over others. Different prompt sets could yield substantially different results.</p>

<p><strong>Evaluation Challenges</strong>: The subjective nature of creative quality makes definitive assessment difficult. Our 120 preference comparisons represent a small sample of possible reader preferences.</p>

<p><strong>Convergence Risks</strong>: Extended evolution could lead to homogenization, where stories converge on particular “winning” formulas rather than maintaining true creative diversity. We observed early signs of this in later generations.</p>

<h3 id="beyond-creative-writing">Beyond Creative Writing</h3>

<p>The Alpha Writing framework extends far beyond narrative fiction. We’ve already employed it in drafting sections of this paper, demonstrating its versatility across writing domains. The approach can be adapted for:</p>

<p><strong>Targeted Generation</strong>: By incorporating specific rubrics, Alpha Writing can optimize individual components of larger works—generating compelling introductions, crafting precise technical explanations, or developing persuasive conclusions. This granular control enables writers to iteratively improve specific weaknesses in their work.</p>

<p><strong>Domain-Specific Applications</strong>: The framework naturally adapts to technical documentation, academic writing, marketing copy, and other specialized formats. Each domain simply requires appropriate evaluation criteria and judge training.</p>

<p><strong>Model Enhancement</strong>: Perhaps most significantly, Alpha Writing offers a systematic approach to improving language models’ general writing capabilities. By generating diverse, high-quality training data through evolutionary refinement, we can potentially bootstrap better foundation models—creating a virtuous cycle where improved models generate even better training data for future iterations.</p>

<p>This positions Alpha Writing not just as a tool for end-users, but as potentially a fundamental technique for advancing the writing capabilities of AI systems themselves.</p>

<h3 id="conclusion">Conclusion</h3>

<p>Alpha Writing demonstrates that creative tasks can benefit from systematic inference-time compute scaling through evolutionary approaches. Our results show consistent improvements over both baseline generation and sequential prompting methods, suggesting that the apparent intractability of scaling compute for creative domains may be addressable through appropriate algorithmic frameworks.</p>

<p><strong>Code Repository</strong>: <a href="https://github.com/tamassimonds/AlphaEvolveWritting">AlphaWrite on GitHub</a></p>

<h3 id="citation">Citation</h3>

<div class="language-bibtex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nc">@article</span><span class="p">{</span><span class="nl">simonds2025alphawrite</span><span class="p">,</span>
  <span class="na">title</span><span class="p">=</span><span class="s">{AlphaWrite: Inference Time Compute Scaling for Writing}</span><span class="p">,</span>
  <span class="na">author</span><span class="p">=</span><span class="s">{Simonds, Toby}</span><span class="p">,</span>
  <span class="na">journal</span><span class="p">=</span><span class="s">{Tufa Labs Research}</span><span class="p">,</span>
  <span class="na">year</span><span class="p">=</span><span class="s">{2025}</span><span class="p">,</span>
  <span class="na">month</span><span class="p">=</span><span class="s">{June}</span><span class="p">,</span>
  <span class="na">url</span><span class="p">=</span><span class="s">{https://github.com/tamassimonds/AlphaEvolveWritting}</span>
<span class="p">}</span>
</code></pre></div></div>

        
    </div>

    
</article>
 
    </main>
</body>
</html> 